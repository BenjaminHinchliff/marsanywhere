\chapter{Background} \label{sec:background}

Our work builds on a variety of prior work both by NASA in relation to the Mars
data used, and on exsiting cross-view pipelines on earth.

\section{The Perseverance (M2020) Rover}

The Mars 2020 mission is the most recent rover mission to Mars, deploying the
Perseverance rover. The Perseverance rover is structurally very similar to the
more well-known Curiosity rover, of the Mars Science Labratory (MSL) mission. It
was launched for the purposes of broadening the knowledge of the Martian surface
and to explore the Jezero crater region. It also has hardware to collect samples
within the rover and in dead drops for the purposes of eventually being returned
to earth with the Mars Sample Return (MSR/SRL) mission, or with a future
mission.

The Perseverance rover makes a number of changes to the sensor suite included on
the rover. Relavent to this work, both the resolution and bit depth of the
primary navigation cameras on the mast of the rover (Navcam) were upgraded,
having a 1600x1200 pixel resolution ($\sim$2 megapixels) and being in full color
\cite{M2020_Willford2018}. This is in constrast to the Curiosity rover, which
has 1024x1024 pixel resolution with only black and white
\cite{MSLNavcam_Maki2012}. This has the impact that the data processing pipeline
used by this work, as well as the machine learning portion, does not generalize
between the two missions. As such, the newer and higher-quality Perseverance
rover imagery has been chosen for usage in this work.

\section{HiRISE}

The High Resolution Imaging Science Experiment, aka HiRISE mission, is a mission
focused on high resolution imaging of places of interest on the martian surface.
Initally launched in 2005, the orbiter arrived at Mars in 2006, where it has
been operaating ever since (long since exceeding its initial (prime) mission
length of 2 years) \cite{HiRISE_McEwen2007}. All orbital imagery used in this
work was aquired from imagery captured by the HiRISE orbiter, later processed
and released as part of the Mars Trek application  \cite{MarsTrek_Law2017}.

\section{VICAR Files}

The Video Image Communication and Retrieval (VICAR) file format is a open
standard internal file format developed by JPL. It's used internally for a
number of missions, starting from Voyager up and until the Perseverance
rover (M2020) mission. It allows for the storage of arbitrary 2d matrix data in
both integer and floating point pixel formats.

Perhaps due to the age of the format, it utilizes slightly different terminology
that is common in image formats today. A VICAR file is organized into one or
more "bands" of data, where each band is has a specified data type. Each band
has a given number of "lines", the height of the image band, and a given number
of "samples" per liine, the image width. The start of the file also includes a
labels, text data used both for specifying image metadata (dimensions, pixel
format, etc), and for arbitrary metadata. In the case of M2020 Perseverance
imagery, notable metadata includes the location of the image (relative to the
site frame), and the time the image was taken \cite{VICAR}.

\section{GeoTIFF}

GeoTIFF is an open metadata standard for storing georeferencing information
inside the already widely-utilized TIFF file format. It allows for a TIFF file
to contain coordinate system information and information on the projection used
by a given GeoTIFF file to allow for usage in precise geolocation. GeoTIFF is
also widely supported, with it being a primary file format used by GDAL, an open
soruce suite of tools for manipulation geographic data.
[\url{https://docs.ogc.org/is/19-008r4/19-008r4.html}]

In this work, the USGS sattelite data from HiRISE was in GeoTIFF format, and
manipuulated by GDAL \cite{USGS_Mars2020_TRN_HiRISE_2020, GDAL_2025}.

\section{Diffusion Models}

Diffusion models create a model that is trained to denoise images by
artificially adding progressively worse noise during training. After training,
random noise can be fed into the model and the model will progressively remove
the noise and in the process effectively "hallucinate" an image that aligns with
its training data. This process can be guided towards specific outcomes using a
process called conditioning in the form of mechanisms like cross-attention that
"teach" the network to focus on specific values \cite{Diffusion_Ho2020}.

In the context of cross-view synthesis, a more complex version of the technique
called latent diffusion (which operates on the latent space) is used
\cite{LatentDiffusion_Rombach2022}.

\section{ControlNet}

ControlNets are a technique to allow for effective modification of an already
pretrained model to allow for the specilization of an existing foundation model
to a new application [EXPAND] \cite{ControlNet_Zhang2023}.
