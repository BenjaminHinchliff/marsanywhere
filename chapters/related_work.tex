\chapter{Related Work} \label{sec:related_work}

\section{Aerial Ground Co-location/Geolocation}

A closely related field of research is that of cross view geo-localization,
specifically, aerial ground co-location, which consists of localizing ground
imagery based on corresponding aerial imagery. This has extensive application
varying from self-driving cars to robotics.

The state of the art consists of many different approaches varying from
traditional feature matching techniques \cite{castaldo2015featurematching} to
ground to aerial synthesis with feature matching \cite{regmi2019g2amatching}.

Notable to this work, similar techniques have also been applied on mars with the
roll out of Global Localization techniques for Perseverance rover operation,
utilizing the aerial view projected orthomoasics and template matching
\cite{verma2024globalloco}.

\section{Satellite-to-street View Synthesis}

Satellite to street view synthesis for Earth imagery is a well-studied problem,
with a focus on applications to urban areas, as these tend to be the most
difficult. With sufficiently large dataset, direct image to image translation
can produce decent results, however on earth the additional complexity of the
imagery can create a number of issues. For example, if the model lacks an
understanding of the different types of structures in the imagery, it can
misidentify them and then subsequently mispredict them in the imagery. For
example, misunderstanding a helipad as a road or the like. To help mitigate
these issues, it's common to introduce additional semantic information to
attempt to categorize the different types of terrain, such as grass, road,
trees, which helps improve model performance \cite{zhai2017groundlayout,
tang2019selectiongan}.

Another common avenue of research is to focus on various reprojection
techniques, such as projecting to a birds eye view \cite{ye2025bev}, or polar
transformation \cite{toker2021dte}. Another notable example is differentiable
reprojection techniques such as in the work of
\citeauthor{shi2022geopanoramasynth} where a learnable reprojection module is
used to relate the satellite and ground views \cite{shi2022geopanoramasynth}.

\section{Diffusion Models}

Diffusion models create a model that is trained to progressively denoise images
by artificially adding progressively worse noise during training. After
training, random noise can be fed into the model and the model will
progressively remove the noise and in the process almost replicate an image that
aligns with its training data. This process can be conditioned towards specific
outcomes using conditioning in the form of mechanisms like cross-attention
\cite{Diffusion_Ho2020}.

In the context of cross-view synthesis, a more complex version of the technique
called latent diffusion (which operates on the latent space) is used
\cite{LatentDiffusion_Rombach2022}.

Our work is specifically based upon Stable Diffusion 2.1
\cite{Rombach_2022_CVPR}, a high quality open source U-Net based latent
diffusion model.

\section{ControlNet}

ControlNets are a technique to allow for fine-tuning an existing model to be
conditioned using new inputs, without permanently modifying the original model
and with far less compute than training the entire model. This is done by
creating a trainable copy of the downward convolutional layers in the U-Net
backbone which are then added back into the model progressively through zero
convolution layers, as shown in figure~\ref{fig:controlnet-arch}
\cite{ControlNet_Zhang2023}.

\begin{figure}

\includegraphics[width=\linewidth]{sd.png}

\caption{Diagram of ControlNets showing how a trainable copy is progressively
added into the U-Net backbone of stable diffusion to allow for model fine-tuning
without changing the original model \cite{ControlNet_Zhang2023}.}

\label{fig:controlnet-arch}

\end{figure}
