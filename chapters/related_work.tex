\chapter{Related Work} \label{sec:related_work}

\section{Aerial Ground Co-location/Geolocation}

A closely related field of research is that of cross view geo-localization,
specifically, aerial ground co-location, which consists of localizing ground
imagery based on corresponding aerial imagery. This has extensive application
varying from self-driving cars to robotics.

The state of the art consists of many different approaches varying from
traditional feature matching techniques \cite{castaldo2015featurematching} to
ground to aerial synthesis with feature matching \cite{regmi2019g2amatching}.

Notable to this work, similar techniques have also been applied on mars with the
roll out of Global Localization techniques for Perseverance rover operation,
utilizing the aerial view projected orthomoasics and template matching
\cite{verma2024globalloco}.

\section{Satellite-to-street View Synthesis}

Satellite to street view synthesis for Earth imagery is a well-studied problem,
with a focus on applications to urban areas, as these tend to be the most
difficult. With sufficiently large dataset, direct image to image translation
can produce decent results, however on earth the additional complexity of the
imagery can create a number of issues. For example, if the model lacks an
understanding of the different types of structures in the imagery, it can
misidentify them and then subsequently mispredict them in the imagery. For
example, misunderstanding a helipad as a road or the like. To help mitigate
these issues, it's common to introduce additional semantic information to
attempt to categorize the different types of terrain, such as grass, road,
trees, which helps improve model performance \cite{zhai2017groundlayout, tang2019selectiongan}.

Another common avenue of research is to focus on various reprojection techniques, such as projecting to a birds eye view \cite{ye2025bev}, or polar
transformation \cite{toker2021dte}. Another notable example is differentiable reprojection techniques such as in the work of \citeauthor{shi2022geopanoramasynth} where a learnable reprojection module is used to relate the satellite and ground views \cite{shi2022geopanoramasynth}.

\section{Diffusion Models}

Diffusion models create a model that is trained to denoise images by
artificially adding progressively worse noise during training. After training,
random noise can be fed into the model and the model will progressively remove
the noise and in the process effectively "hallucinate" an image that aligns with
its training data. This process can be guided towards specific outcomes using a
process called conditioning in the form of mechanisms like cross-attention that
"teach" the network to focus on specific values \cite{Diffusion_Ho2020}.

In the context of cross-view synthesis, a more complex version of the technique
called latent diffusion (which operates on the latent space) is used
\cite{LatentDiffusion_Rombach2022}.

\section{ControlNet}

ControlNets are a technique to allow for effective modification of an already
pretrained model to allow for the specilization of an existing foundation model
to a new application [EXPAND] \cite{ControlNet_Zhang2023}.
