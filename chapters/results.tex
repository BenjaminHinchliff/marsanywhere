\chapter{Results} \label{sec:results}

We evaluate the trained model on the 20\% test split of the dataset on metrics
inherited from past cross view diffusion work, namely Sharpness Difference,
PSNR, SSIM, FID, and KID \cite{li2024crossviewdiff}.

\section{Metrics}

Note that for the formulation of these metrics we assume image data is a matrix
of float data in the range [0.0, 1.0], inclusive.

\subsection{Sharpness Difference}

Sharpness Difference (SD) quantifies the difference between the gradients of the
predicted and initial images. Simply put, it measures the loss of sharpness in
an image \cite{mathieu2016bmse}.

This is calculated as the difference between the sum of the horizontal and
vertical gradients as calculated via simple difference as follows:

\begin{equation}
    \SD(I_g, I'_g) = 10 \log_{10} \frac{1}{\GS(I_g, I'_g)}
\end{equation}

where

\begin{equation}
    \GS(I_g, I'_g)
        = \frac{1}{m n} \sum_{i=1}^{m-1} \sum_{j=1}^{n-1} \left| (\nabla_i I_g + \nabla_j I_g) - (\nabla_i I'_g + \nabla_j I'_g) \right|
\end{equation}

and

\begin{align*}
    \nabla_i I &= \left| I_{i,j} - I_{i-1,j} \right| \\
    \nabla_j I &= \left| I_{i,j} - I_{i,j-1} \right|
\end{align*}

We make a minor modification to this formulation, using central difference as
opposed to simple difference, but don't believe this should significantly affect
the results.

\subsection{Peak Signal to Noise Ratio}

Peak Signal to Noise Ratio (PSNR) is a simple and ubiquitous metric to measure
image quality loss. Intuitively, it can be described as the ratio between the
true image and the noise (errors) present in the "reconstructed" (synthesized)
images. Formally, this is expressed as follows:

\begin{equation}
    \mathrm{PSNR}(I_g, I'_g) = 10 \log_{10} \frac{1}{\mathrm{MSE(I_g, I'_g)}}
\end{equation}

where

\begin{equation}
    \mathrm{MSE}(I_g, I'_g) = \frac{1}{m n} \sum_{i=1}^{m-1} \sum_{j=1}^{n-1} \left( I_{i, j} - I'_{i, j} \right)^2
\end{equation}

\subsection{Structural Similarity}

Structural similarity is a composite metric that attempts to better quantify how
perceptually similar two images are. It consists of three components. The
luminance component $l$ attempts to quantify where luminance masking might
occur, as in when differences in intensity are less visible due to being in a
bright region. The contrast component $c$ does similar for contrast masking,
where changes in contrast might be less visible in highly textured regions.
Lastly the structural term, $s$, where the standard deviation is used as a
vector correlation to compare the structure of the two images
\cite{wang2004ssim}.

This leads to the following formulation:

\begin{equation}
    \mathrm{SSIM}(I, I') = l(I, I')^\alpha \cdot c(I, I')^\beta \cdot s(I, I')^\gamma
\end{equation}

where

\begin{align}
    l(I, I') &= \frac{2 \mu_{I} \mu_{I'} + c_1}{\mu_{I}^2 + \mu_{I'}^2 + c_1} \\
    c(I, I') &= \frac{2 \sigma_{I} \sigma_{I'} + c_2}{\sigma_{I}^2 + \sigma_{I'}^2 + c_2} \\
    s(I, I') &= \frac{\sigma_{I I'} + c_3}{\sigma_{I} \sigma_{I'} + c_3}
\end{align}

where

\begin{itemize}
    \item $\mu_I$ is the pixel sample mean of $I$.
    \item $\mu_{I'}$ is the pixel sample mean of $I'$.
    \item $\sigma_{I}^2$ is the sample variance of $I$.
    \item $\sigma_{I'}^2$ is the sample variance of $I'$.
    \item $\sigma_{I I'}$ is the sample covariance of $I$ and $I'$.
    \item $c_1$, $c_2$, and $c_3$ are stabilizing constants calculated as $c_n =
    (k_n L)^2$, where L is the dynamic range (maximum luminance) of the pixel
    values.
\end{itemize}

In our work, we use the special cased version of $\mathrm{SSIM}$, with $k_1 =
0.01$, $k_2 = 0.03$ and $c_3 = c_2 / 2$.

\subsection{Fr\'echet Inception Distance}

Fr\'echet Inception Distance is a model based evaluation metric that compares
the multivariate normal distributions estimated by Inception v3 features. The
more similar the distributions, the smaller the distance between them, and the
lower the metric \cite{Szegedy_2016_CVPR}.

Formally, it defined as:

\begin{equation}
    \mathrm{FID(I, I')} = \| \mu_I - \mu_{I'} \|^2 + \tr(\Sigma_I + \Sigma_{I'} - 2(\Sigma_I \Sigma_{I'})^{\frac{1}{2}})
\end{equation}

where

\begin{itemize}
	\item $\mathcal{N}(\mu_I, \Sigma_I)$ is the multivariate normal distribution
	estimated from Inception v3 based on ground truth imagery.
	\item $\mathcal{N}(\mu_{I'}, \Sigma_{I'})$ is the multivariate normal
	distribution estimated from Inception v3 based on synthesized imagery.
\end{itemize}

\subsection{Kernel Inception Distance}

Kernel inception distance is a metric similar to FID, also based on the
Inception v3 model. Unlike FID, however, it computes the difference using
Maximum Mean Discrepancy (MMD) with a polynomial kernel function
\cite{bi≈Ñkowski2018demystifying}.

Formally this is expressed as:

\begin{equation}
    \mathrm{KID} = \mathrm{MMD}(I, I')^2
\end{equation}

with the polynomial kernel function:

\begin{equation}
    k(x, y) = (\gamma x^T y + c)^d
\end{equation}

where in our work:

\begin{itemize}
    \item $\gamma = 1 / $representation dimension
    \item $c = 1$
    \item $d = 3$
\end{itemize}

\section{Experimental Setting}

We evaluate the models three different models on the test split in three
different levels of masking for comparing the difference between the imagery.

First, we test a basic ControlNet model trained with the raw satellite patch
(\textit{without} being projected to a ground view) as the prior, which we refer
to as no projection.

After this, we test our models based on a prior projected to a ground view as
outlined in \cref{fig:data-pipeline}. We evaluate a model with masking only
applied to the conditioning imagery based on to the ground truth imagery as
outlined in section~\ref{sec:loss-masking}, referred to as ground masking.
Lastly, we evaluate the model with masking as outlined in
section~\ref{sec:mut-masking}, referred to as joint masking.

We also include a baseline for a model that always predicts the mean of the
ground imagery in the training set. This mean ground imagery is shown in
\cref{fig:mean-test-image} and is referred to as the mean baseline.

\begin{figure}
    \includegraphics[width=\linewidth]{mean_test_image.jpg}
    \caption{Mean test image used as a baseline to compare our models against.
    This is the channel-wise mean of the ground truth pixel data in the training set.}
    \label{fig:mean-test-image}
\end{figure}

For the evaluation itself, we evaluate with 3 different masking configurations.
First, we evaluate the models based on the full imagery output as compared with
the ground truth imagery. This serves for the purposes of completeness and
demonstrating that masking does not unduly bias the results. However, it is of
limited usefulness as the results are heavily biased by differences between the
regions where the true imagery is missing data and the model generated imagery,
which was specifically trained to not exhibit this behavior as outlined in
section~\ref{sec:loss-masking}. We will refer to this as unmasked evaluation.

To address this we then evaluate the models while masking to only include data
that exists in the true ground imagery, as in section~\ref{sec:loss-masking}. We
believe this to likely be the most representative evaluation of our techniques.

Lastly, we evaluate the models while limiting the region of interest to only the
joint mask. This is to help determine the effectiveness of our joint masking
techniques from section~\ref{sec:mut-masking}. If these techniques are effective
for better guiding the performance of the model, we should expect comparable or
better performance between the target masking and full masking on this imagery.

\section{Quantitative Evaluation}

As can be seen be seen from tables \ref{tab:unmasked} and \ref{tab:gt-masked},
ground truth masking for the purposes of evaluation while it made significant
differences to the values themselves, didn't change the general trends of
results.

Models trained using both methods of masking struggled to compete against the
baseline on our more traditional (not-learning based) metrics, as seen in
\cref{tab:unmasked,tab:gt-masked,tab:cond-masked}. This is an unfortunate, but
not unsurprising result. These more traditional metrics have littl1
understanding of true photorealism, and rather focus on the absolute difference
between the output image and the ground truth imagery. In addition of the
instability inherent to diffusion model generation, as well as artifacts present
in the training data, the diffusion model at times struggled to reconstruct the
desired aspects of the data. However, our techniques at times came close to
competing with this prior. For instance, in \cref{tab:cond-masked} , ground
masking came within 0.5 dB of the mean baseline. We believe this gap may be able
to be narrowed with more data.

However, on metrics with a greater focus on quantifying higher-level differences
between imagery, as seen in \cref{tab:unmasked,tab:gt-masked,tab:cond-masked},
our models perform much better than the baseline. Namely, the FID and KID of our
models represent an order of magnitude improvement over our na\"ive baseline.
This is likely because these metrics focus on the how the distribution of higher
level features (as extracted by classifier networks) compare, on which our
models can be visually seen to perform well at producing imagery resembling that
of Mars.

We were pleased to see that the reprojection of the imagery improved the quality
of results not just qualitatively, but quantitatively as well. For instance in
\cref{tab:gt-masked} the reprojected model utilizing ground masking performed a
bit better on all metrics as compared with the no reprojection model. This trend
holds in both \cref{tab:unmasked,tab:cond-masked}. This strongly indicates that
reprojecting the aerial view imagery to a simulated ground view improved the
quality of our results.

Lastly, our experiments with full masking have yielded mixed results. While it,
unsurprisingly, performs worse in the full scope evaluation of
\cref{tab:unmasked,tab:gt-masked}, we were surprised to see that even
in the fully masked evaluation shown in \cref{tab:cond-masked}, it did not
show better performance. We believe that this may haver been due to
correspondence between the conditioning and ground truth imagery, while it is
good, tends to have a slight vertical translation offset. This means that
relevant sections of the ground truth data may have been excluded during
training.

\begin{table}

\input{figures/table-unmasked.tex}

\caption{Mean SD, PSNR, SSIM, FID, and KID across the test set without any masking.}
\label{tab:unmasked}

\end{table}

\begin{table}

\input{figures/table-target.tex}

\caption{
Mean SD, PSNR, SSIM, FID, and KID across the test set with masking to only data
present in the true ground imagery.
}
\label{tab:gt-masked}

\end{table}

\begin{table}

\input{figures/table-full.tex}

\caption{
Mean SD, PSNR, SSIM, FID, and KID across the test set with masking to only data
present in \textbf{both} the projected conditioning imagery and masked ground imagery.
}
\label{tab:cond-masked}

\end{table}

\section{Qualitative Evaluation}

While the results are generally impressive, they all contain the major known
artifact is that the model attempts to reconstruct the Perseverance rover within
the imagery. This fact alone may have been why full image masking was met with
limited success, as the rover often occluded the overlapping portion of the
image. This leads the model to predict the rover more often than not, which is
not our goal in producing the imagery. Other minor artifacts such as attempting
replicate stitch lines between images are also present.

Consistent with our goals for this work, our model appears to demonstrate some
knowledge of reconstructing from the terrain in the prior, as shown in
\cref{fig:no-mask-good}. In other cases where it struggles it can be seen to
"misunderstand" specific terrain and textures and struggle to reconstruct it,
but still produces impressive results, as shown in \cref{fig:no-mask-bad}. Full
test results can be found in appendix~\ref{sec:full-test-results}.

\begin{figure}

\begin{tabular}{c c c}
    Conditioning Image & Ground Truth & No Mask Model Result \\
    \includegraphics[width=0.33\linewidth]{vis/N_LRGB_0533XRZS_0261222_CYL_L_AUTOGENJ01.jpg} &
    \includegraphics[width=0.33\linewidth]{ground_true/N_LRGB_0533XRZS_0261222_CYL_L_AUTOGENJ01.jpg} &
    \includegraphics[width=0.33\linewidth]{no_mask_assembled/N_LRGB_0533XRZS_0261222_CYL_L_AUTOGENJ01.jpg} \\
    \includegraphics[width=0.33\linewidth]{vis/N_LRGB_0627XRZS_0301172_CYL_L_AUTOGENJ02.jpg} &
    \includegraphics[width=0.33\linewidth]{ground_true/N_LRGB_0627XRZS_0301172_CYL_L_AUTOGENJ02.jpg} &
    \includegraphics[width=0.33\linewidth]{no_mask_assembled/N_LRGB_0627XRZS_0301172_CYL_L_AUTOGENJ02.jpg}
\end{tabular}

\caption{Example cases where the no mask model performs well, successfully replicating
both the ground shape contours (as can be seen with the mounds on the left and
right), and the general texture of the ground. Specific rocks however, were not synthesized accurately.}
\label{fig:no-mask-good}

\end{figure}

\begin{figure}

\begin{tabular}{c c c}
    Conditioning Image & Ground Truth & No Mask Model Result \\
    \includegraphics[width=0.33\linewidth]{vis/N_LRGB_1114_RZS_0512990_CYL_L_AUTOGENJ01.jpg} &
    \includegraphics[width=0.33\linewidth]{ground_true/N_LRGB_1114_RZS_0512990_CYL_L_AUTOGENJ01.jpg} &
    \includegraphics[width=0.33\linewidth]{no_mask_assembled/N_LRGB_1114_RZS_0512990_CYL_L_AUTOGENJ01.jpg} \\
    \includegraphics[width=0.33\linewidth]{vis/N_LRGB_1177_RZS_0540000_CYL_L_AUTOGENJ02.jpg} &
    \includegraphics[width=0.33\linewidth]{ground_true/N_LRGB_1177_RZS_0540000_CYL_L_AUTOGENJ02.jpg} &
    \includegraphics[width=0.33\linewidth]{no_mask_assembled/N_LRGB_1177_RZS_0540000_CYL_L_AUTOGENJ02.jpg}
\end{tabular}

\caption{Example cases where the no mask model struggled with synthesis. In the first
row, the model misunderstood the left ridge from and failed to accurately
reconstruct it in the imagery. In the second row, the model failed to accurately
synthesize the ground texture accurately}
\label{fig:no-mask-bad}

\end{figure}

\section{Limitations}

Due to issues inherent to the data, this work has a couple of limitations that
should be taken into consideration as to the generalization and quality of our
results.

First, the data contains multiple images from similar or sometimes even the
exact same location. While having multiples of these images is still useful for
training and reconstruction, it presents issues when generating the train and
test splits. Specifically, there can effectively be data leakage (due to very
similar ground imagery) between the train and test splits when simple random
sampling is used. Since this issue was only discovered after model training,
this issue has not been mitigated in this work, but could be for future work.

Second, is that because the mosaics are assembled at a different time than the
actual imagery is taken, there is the potential for difference between the
location at which the image mosaic was assembled. While this sounds like a major
issue at first blush, it should be noted that this is actually quite minor. Due
to the slow speed of the rover, even rather substantial differences in time are
likely to be quite close in terms of the satellite view.

Lastly, the satellite imagery can, and often does, overlap significantly due to
the rover's slow speed. While we did attempt to account for this by adjusting
the limiting size of the satellite patches, there is no perfect solution to this
problem. As a result, this is another factor that may have lead to spurious
correlation between the train and test sets, as well as within the dataset
itself.

As a result of these limitations, while measures have been taken to attempt to
allow the results to be generalized to other regions of mars, the degree to
which the model can be generalized has yet to be tested.
